```{r}
# install.packages('dplyr');
library(dplyr);
```

### Загрузка и просмотр датасета

```{r}
customers = read.csv('Customers.csv', sep=';')
head(customers)
```

### Удаление `customer_id` и преведение `gender` к численному виду

```{r}
customers = customers %>% select(-customer_id)

customers$gender = as.numeric(as.character(factor(customers$gender, levels=c('Male', 'Female'), labels = c('1', '0'))))

head(customers)
```

### Локтевой метод

```{r}
tot.withinss <- vector(mode="character", length=10)
for (i in 1:10){
 customers_cluster <- kmeans(customers, center=i, nstart=20)
 tot.withinss[i] <- customers_cluster$tot.withinss
}
plot(1:10, tot.withinss, type="b", pch=19)
```

```{r}
N_CLUSTERS = 3
```

### Кластеризация методом К-средних

```{r}
set.seed(42);
customers_cluster = kmeans(customers, center=N_CLUSTERS)
plot(
  x=customers$age,
  y=customers$spending_score,
  col=customers_cluster$cluster,
  pch=customers_cluster$cluster
)
points(
  x=customers_cluster$centers[,'age'],
  y=customers_cluster$centers[,'spending_score'],
  pch=5,
  lwd=5,
  col='red'
)
```

### Иерархическая кластеризация

```{r}
distance = dist(customers)
hclusters <- hclust(distance, method = "average")
```

```{r}
plot(x = hclusters)
rect.hclust(hclusters, k=3, border = 2:4)
```

```{r}
cuts <- cutree(tree = hclusters, k = 3)
plot(
  x=customers$age,
  y=customers$spending_score,
  col=cuts+1,
  pch=cuts 
)

```

# Вопросы

### 1. Почему данные необходимо преобразовать в числовые значения?

Для корректной работы моделей данные необходимо преобразовать в числовые значения, так как модели не умеют работать со строковыми данными.

### 2. Объяснить, для чего используется функция `set.seed(42)` в работе.

Данная функция используется для установки начального числа (сида) для генерации псевдо-случайных чисел. Это необходимо для того, чтобы при использовании этих чисел моделями они каждый раз были одинаковыми, и результат работы был воспроизводим. `42` - потому, что это ответ на все вопросы.

### 3. Объяснить выбранное количество кластеров в иерархической кластеризации.

Было выбрано 3 кластера, так как на дендрограмме видно, что линия, разделяющая кластеры, проходит относительно далеко от нижних уровней дерева.
